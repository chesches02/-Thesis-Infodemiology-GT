{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_adjacency_matrices(end_dates, data_dir = './data/processed/d_corr/GT', thresholds = [0.5,0.4,0.6], is_srd = False):\n",
    "    for threshold in thresholds:\n",
    "        adj_mat_subfolder = f\"threshold-{threshold}\"\n",
    "        adj_mat_path = f'{data_dir}/adjacency_matrices/{adj_mat_subfolder}'\n",
    "        cor_mat_path = f'{data_dir}'\n",
    "\n",
    "        if not os.path.exists(adj_mat_path):\n",
    "            os.makedirs(adj_mat_path, exist_ok=True)\n",
    "\n",
    "        for end_date in end_dates:\n",
    "            corr_df = pd.read_csv(f'{cor_mat_path}/{end_date}.csv', index_col=0)\n",
    "            \n",
    "            # Convert all columns (except index) to numeric, errors='coerce' will convert non-numeric values to NaN\n",
    "            corr_df = corr_df.apply(pd.to_numeric, errors='coerce')\n",
    "            \n",
    "            # Replace NaN with 0 to avoid comparison issues\n",
    "\n",
    "            corr_df = corr_df.fillna(0)\n",
    "\n",
    "            adjacency_matrix = np.where(corr_df.values >= threshold, 1, 0)\n",
    "\n",
    "            corr_df.iloc[:, :] = adjacency_matrix\n",
    "            \n",
    "            corr_df.to_csv(f'./data/processed/d_corr_adjacency_matrices/threshold-{threshold}/{end_date}.csv')\n",
    "            corr_df.to_pickle(f'./data/processed/d_corr_adjacency_matrices/threshold-{threshold}/{end_date}.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For untreated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_df = pd.read_csv('./data/processed/gt.csv', parse_dates=['date'])\n",
    "dates = gt_df['date'].dt.date\n",
    "del gt_df\n",
    "\n",
    "window_width = 30\n",
    "\n",
    "selection_index_array = [(index, index + window_width) for index in range(len(dates) - window_width)]\n",
    "end_dates = [dates[index] for (_, index) in selection_index_array]\n",
    "len(end_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_save_adjacency_matrices(end_dates=end_dates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For treated (SRD) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/processed/[03]srd_out/srd_out.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m srd_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/processed/[03]srd_out/srd_out.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dates \u001b[38;5;241m=\u001b[39m srd_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m srd_df\n",
      "File \u001b[1;32mc:\\Users\\Cheska Hung\\Desktop\\Thesis-GT Infodemiology\\-Thesis-Infodemiology-GT\\covid-19-google-trends-network\\.venv\\Lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Cheska Hung\\Desktop\\Thesis-GT Infodemiology\\-Thesis-Infodemiology-GT\\covid-19-google-trends-network\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/processed/[03]srd_out/srd_out.pkl'"
     ]
    }
   ],
   "source": [
    "srd_df = pd.read_pickle('./data/processed/[03]srd_out/srd_out.pkl')\n",
    "dates = srd_df['date'].values\n",
    "del srd_df\n",
    "\n",
    "window_width = 30\n",
    "\n",
    "selection_index_array = [(index, index + window_width) for index in range(len(dates) - window_width)]\n",
    "end_dates = [dates[index] for (_, index) in selection_index_array]\n",
    "len(end_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_save_adjacency_matrices(end_dates=end_dates, is_srd = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flu: 48\n",
      "cough: 36\n",
      "fever: 27\n",
      "headache: 0\n",
      "lagnat: 17\n",
      "rashes: 0\n",
      "sipon: 18\n",
      "ubo: 14\n",
      "ecq: 23\n",
      "face-shield: 11\n",
      "Frontliners: 3\n",
      "masks: 0\n",
      "Quarantine: 38\n",
      "social-distancing: 31\n",
      "work-from-home: 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "def count_triads_per_term(directory):\n",
    "    triad_counts = {}\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(directory, file), index_col=0)\n",
    "        words = df.columns.tolist()\n",
    "        \n",
    "        # Create a dictionary to store connections\n",
    "        connections = {word: set() for word in words}\n",
    "\n",
    "        # Populate connections based on the DataFrame\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i + 1, len(words)):\n",
    "                if df.iloc[i, j] == 1:  # Check if there's a connection\n",
    "                    connections[words[i]].add(words[j])\n",
    "                    connections[words[j]].add(words[i])\n",
    "\n",
    "        # Count triads for each word\n",
    "        for word in words:\n",
    "            triad_count = 0\n",
    "            for word1, word2 in combinations(connections[word], 2):\n",
    "                if word1 in connections and word2 in connections[word1]:\n",
    "                    triad_count += 1\n",
    "            \n",
    "            # Store the count for the word\n",
    "            if word in triad_counts:\n",
    "                triad_counts[word] += triad_count // 3  # Each triad is counted 3 times\n",
    "            else:\n",
    "                triad_counts[word] = triad_count // 3\n",
    "\n",
    "    return triad_counts\n",
    "\n",
    "# Usage\n",
    "directory_path = r'C:\\Users\\Cheska Hung\\Desktop\\Thesis-GT Infodemiology\\-Thesis-Infodemiology-GT\\covid-19-google-trends-network\\scripts\\construct_network\\data\\processed\\RSV_adjacency_matrices\\threshold-0.5'\n",
    "triad_counts = count_triads_per_term(directory_path)\n",
    "\n",
    "# Print the total number of triads for each search term\n",
    "for term, count in triad_counts.items():\n",
    "    print(f'{term}: {count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('network')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78e082af443e87a1620ec82c2fce242a18414551eac914730d5b559982cbecfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
